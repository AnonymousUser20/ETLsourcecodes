{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e04dabe4-61e0-4c68-9b4f-00cac8c73192",
      "metadata": {
        "id": "e04dabe4-61e0-4c68-9b4f-00cac8c73192"
      },
      "source": [
        "# IMPORT MODULES (always run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZvBseESck711",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvBseESck711",
        "outputId": "d56049ee-9a7b-4716-bfd0-63080889543f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bb07a1a-2809-483e-ae14-f7f4877b112b",
      "metadata": {
        "id": "6bb07a1a-2809-483e-ae14-f7f4877b112b"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import csv\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8fa9731-2393-40f7-bece-cdf37ec7bcb4",
      "metadata": {
        "id": "f8fa9731-2393-40f7-bece-cdf37ec7bcb4"
      },
      "source": [
        "# arXiv Search Script"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following scripts are written to extract first date of visibility of every paper on arXiv, and the extracted data are stored as CSVs. arXiv is a static website, hence, data can be scraped from HTML tags of the website."
      ],
      "metadata": {
        "id": "V3iGdCfArTcF"
      },
      "id": "V3iGdCfArTcF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0e40290-bebe-470d-95ad-65a822de7ec1",
      "metadata": {
        "id": "e0e40290-bebe-470d-95ad-65a822de7ec1"
      },
      "outputs": [],
      "source": [
        "#load the conference submission data from json : ICML/ICLR\n",
        "with open(\"ICML/ICML2023.json\", 'r') as file:\n",
        "    data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4669b0a3-8342-456a-a0e1-3e8555a6c735",
      "metadata": {
        "id": "4669b0a3-8342-456a-a0e1-3e8555a6c735"
      },
      "outputs": [],
      "source": [
        "with open (\"arxiv DATA/ICML/ICML2023arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "\n",
        "    for i in range (len(data)):\n",
        "        paper_title = data[i]['name']\n",
        "        paper = paper_title.replace(\" \",\"+\")\n",
        "        search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "        response = requests.get(search_query)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # <p> tag that contains the submission info\n",
        "        submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "        date = \"Not Found\"\n",
        "\n",
        "        if submission_info:\n",
        "            text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "            # find v1 submitted date first\n",
        "            v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "            if v1_match:\n",
        "                date = v1_match.group(1).strip()\n",
        "            else:\n",
        "                # If not found, originally announced date\n",
        "                announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                if announced_match:\n",
        "                    date = announced_match.group(1).strip()\n",
        "\n",
        "        writer.writerow([paper_title, date])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F652knQp5pUr",
      "metadata": {
        "id": "F652knQp5pUr"
      },
      "source": [
        "## AAAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o50zwTzf5pUr",
      "metadata": {
        "id": "o50zwTzf5pUr"
      },
      "outputs": [],
      "source": [
        "years = [2023, 2024, 2025]\n",
        "for year in years:\n",
        "  df = pd.read_csv(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/AAAI/AAAI{year}.csv\")\n",
        "  titles = df[\"Title\"].to_list()\n",
        "  with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/AAAI/AAAI{year}arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "      writer=csv.writer(f)\n",
        "      writer.writerow([\"Title\",\"Submission Date\"])\n",
        "\n",
        "      for i in range (len(titles)):\n",
        "          paper_title = titles[i]\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zwlvPOVMlX-d",
      "metadata": {
        "id": "zwlvPOVMlX-d"
      },
      "source": [
        "## KDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3vyQkgGgmN69",
      "metadata": {
        "id": "3vyQkgGgmN69"
      },
      "outputs": [],
      "source": [
        "year = 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M5788W3umOS7",
      "metadata": {
        "id": "M5788W3umOS7"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/KDD/KDD{year}papers.csv\")\n",
        "titles = df[\"Title\"].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ncVTVfGElZwa",
      "metadata": {
        "id": "ncVTVfGElZwa"
      },
      "outputs": [],
      "source": [
        "\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/KDD/KDD{year}arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "\n",
        "    for i in range (len(titles)):\n",
        "        paper_title = titles[i]\n",
        "        paper = paper_title.replace(\" \",\"+\")\n",
        "        search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "        response = requests.get(search_query)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # <p> tag that contains the submission info\n",
        "        submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "        date = \"Not Found\"\n",
        "\n",
        "        if submission_info:\n",
        "            text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "            # find v1 submitted date first\n",
        "            v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "            if v1_match:\n",
        "                date = v1_match.group(1).strip()\n",
        "            else:\n",
        "                # If not found, originally announced date\n",
        "                announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                if announced_match:\n",
        "                    date = announced_match.group(1).strip()\n",
        "\n",
        "        writer.writerow([paper_title, date])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xAevqNna91id",
      "metadata": {
        "id": "xAevqNna91id"
      },
      "source": [
        "## ACL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D88QFTXT93Uz",
      "metadata": {
        "id": "D88QFTXT93Uz"
      },
      "outputs": [],
      "source": [
        "year = 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XT977jJ3-Hbb",
      "metadata": {
        "id": "XT977jJ3-Hbb"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ACL/ACL{year}papers.csv\")\n",
        "titles = df[\"Title\"].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s0mQvCGN-QDz",
      "metadata": {
        "id": "s0mQvCGN-QDz"
      },
      "outputs": [],
      "source": [
        "\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/ACL/ACL{year}arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "\n",
        "    for i in range (len(titles)):\n",
        "        paper_title = titles[i]\n",
        "        paper = paper_title.replace(\" \",\"+\")\n",
        "        search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "        response = requests.get(search_query)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # <p> tag that contains the submission info\n",
        "        submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "        date = \"Not Found\"\n",
        "\n",
        "        if submission_info:\n",
        "            text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "            # find v1 submitted date first\n",
        "            v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "            if v1_match:\n",
        "                date = v1_match.group(1).strip()\n",
        "            else:\n",
        "                # If not found, originally announced date\n",
        "                announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                if announced_match:\n",
        "                    date = announced_match.group(1).strip()\n",
        "\n",
        "        writer.writerow([paper_title, date])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41xap70nNfju",
      "metadata": {
        "id": "41xap70nNfju"
      },
      "source": [
        "## CVPR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UAOlzpmQCHyG",
      "metadata": {
        "id": "UAOlzpmQCHyG"
      },
      "outputs": [],
      "source": [
        "year = 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HxKTVQBmOC3t",
      "metadata": {
        "id": "HxKTVQBmOC3t"
      },
      "outputs": [],
      "source": [
        "with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/CVPR/CVPR{year}.json\", 'r') as file:\n",
        "    data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mjAd3hGCOcqn",
      "metadata": {
        "id": "mjAd3hGCOcqn"
      },
      "outputs": [],
      "source": [
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/CVPR/CVPR{year}arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "\n",
        "    for i in range (len(data)):\n",
        "        paper_title = data[i]['name']\n",
        "        paper = paper_title.replace(\" \",\"+\")\n",
        "        search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "        response = requests.get(search_query)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # <p> tag that contains the submission info\n",
        "        submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "        date = \"Not Found\"\n",
        "\n",
        "        if submission_info:\n",
        "            text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "            # find v1 submitted date first\n",
        "            v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "            if v1_match:\n",
        "                date = v1_match.group(1).strip()\n",
        "            else:\n",
        "                # If not found, originally announced date\n",
        "                announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                if announced_match:\n",
        "                    date = announced_match.group(1).strip()\n",
        "\n",
        "        writer.writerow([paper_title, date])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IodJFREBuipN",
      "metadata": {
        "id": "IodJFREBuipN"
      },
      "source": [
        "## ICLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BfSxOCqQubZb",
      "metadata": {
        "id": "BfSxOCqQubZb"
      },
      "outputs": [],
      "source": [
        "category = \"accepted(oral)\" #[\"accepted(oral)\", \"accepted(poster)\", \"accepted(spotlight)\", \"desk_rejected\", \"rejected\", \"withdrawn\"]\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICLR/ICLR 2025/{category}\")\n",
        "json_count = len(list(directory.glob(\"*.json\")))\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/ICLR/ICLR 2025/{category}/{category}_arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "    for i in range(1, json_count+1):\n",
        "      #change json name with every category update\n",
        "      with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICLR/ICLR 2025/{category}/ICLR2025_oral_page{i}.json\", 'r') as file:\n",
        "        all_content = json.load(file)\n",
        "        for note in all_content['notes']:\n",
        "          paper_title = note['content']['title']['value']\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          #to avoid error 429 : too many requests in short time\n",
        "          if response.status_code == 429:\n",
        "              print(\"Rate limit hit. Waiting...\")\n",
        "              time.sleep(10)  # Wait 10 seconds before retry\n",
        "              response = requests.get(search_query)\n",
        "          response.raise_for_status()\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])\n",
        "\n",
        "        time.sleep(1) #to avoid error 429 : too many requests in short time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ELiC5o53CIvX",
      "metadata": {
        "id": "ELiC5o53CIvX"
      },
      "outputs": [],
      "source": [
        "category = \"accepted(poster)\" #[\"accepted(oral)\", \"accepted(poster)\", \"accepted(spotlight)\", \"desk_rejected\", \"rejected\", \"withdrawn\"]\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICLR/ICLR 2025/{category}\")\n",
        "json_count = len(list(directory.glob(\"*.json\")))\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/ICLR/ICLR 2025/{category}/{category}_arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "    for i in range(1, json_count+1):\n",
        "      #change json name with every category update\n",
        "      with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICLR/ICLR 2025/{category}/ICLR2025_poster_page{i}.json\", 'r') as file:\n",
        "        all_content = json.load(file)\n",
        "        for note in all_content['notes']:\n",
        "          paper_title = note['content']['title']['value']\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          #to avoid error 429 : too many requests in short time\n",
        "          if response.status_code == 429:\n",
        "              print(\"Rate limit hit. Waiting...\")\n",
        "              time.sleep(10)  # Wait 10 seconds before retry\n",
        "              response = requests.get(search_query)\n",
        "          response.raise_for_status()\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])\n",
        "\n",
        "        time.sleep(1) #to avoid error 429 : too many requests in short time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hVPtBcFmCI3e",
      "metadata": {
        "id": "hVPtBcFmCI3e"
      },
      "outputs": [],
      "source": [
        "category = \"accepted(spotlight)\" #[\"accepted(oral)\", \"accepted(poster)\", \"accepted(spotlight)\", \"desk_rejected\", \"rejected\", \"withdrawn\"]\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICLR/ICLR 2025/{category}\")\n",
        "json_count = len(list(directory.glob(\"*.json\")))\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/ICLR/ICLR 2025/{category}/{category}_arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "    for i in range(1, json_count+1):\n",
        "      #change json name with every category update\n",
        "      with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICLR/ICLR 2025/{category}/ICLR2025_spotlight_page{i}.json\", 'r') as file:\n",
        "        all_content = json.load(file)\n",
        "        for note in all_content['notes']:\n",
        "          paper_title = note['content']['title']['value']\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          #to avoid error 429 : too many requests in short time\n",
        "          if response.status_code == 429:\n",
        "              print(\"Rate limit hit. Waiting...\")\n",
        "              time.sleep(10)  # Wait 10 seconds before retry\n",
        "              response = requests.get(search_query)\n",
        "          response.raise_for_status()\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])\n",
        "\n",
        "        time.sleep(1) #to avoid error 429 : too many requests in short time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HcevY12xCI-o",
      "metadata": {
        "id": "HcevY12xCI-o"
      },
      "outputs": [],
      "source": [
        "category = \"desk_rejected\" #[\"accepted(oral)\", \"accepted(poster)\", \"accepted(spotlight)\", \"desk_rejected\", \"rejected\", \"withdrawn\"]\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICLR/ICLR 2025/{category}\")\n",
        "json_count = len(list(directory.glob(\"*.json\")))\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/ICLR/ICLR 2025/{category}/{category}_arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "    for i in range(1, json_count+1):\n",
        "      #change json name with every category update\n",
        "      with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICLR/ICLR 2025/{category}/ICLR2025_{category}_page{i}.json\", 'r') as file:\n",
        "        all_content = json.load(file)\n",
        "        for note in all_content['notes']:\n",
        "          paper_title = note['content']['title']['value']\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          #to avoid error 429 : too many requests in short time\n",
        "          if response.status_code == 429:\n",
        "              print(\"Rate limit hit. Waiting...\")\n",
        "              time.sleep(10)  # Wait 10 seconds before retry\n",
        "              response = requests.get(search_query)\n",
        "          response.raise_for_status()\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])\n",
        "\n",
        "        time.sleep(1) #to avoid error 429 : too many requests in short time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bxSdWWS5CJF4",
      "metadata": {
        "id": "bxSdWWS5CJF4"
      },
      "outputs": [],
      "source": [
        "category = \"reject\" #[\"accepted(oral)\", \"accepted(poster)\", \"accepted(spotlight)\", \"desk_rejected\", \"reject\", \"withdrawn\"]\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICLR/ICLR 2025/{category}\")\n",
        "json_count = len(list(directory.glob(\"*.json\")))\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/ICLR/ICLR 2025/{category}/{category}_arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "    for i in range(1, json_count+1):\n",
        "      #change json name with every category update\n",
        "      with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICLR/ICLR 2025/{category}/ICLR2025_{category}_page{i}.json\", 'r') as file:\n",
        "        all_content = json.load(file)\n",
        "        for note in all_content['notes']:\n",
        "          paper_title = note['content']['title']['value']\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          #error 400\n",
        "          if response.status_code == 400:\n",
        "            continue\n",
        "\n",
        "          #to avoid error 429 : too many requests in short time\n",
        "          if response.status_code == 429:\n",
        "              print(\"Rate limit hit. Waiting...\")\n",
        "              time.sleep(10)  # Wait 10 seconds before retry\n",
        "              response = requests.get(search_query)\n",
        "          response.raise_for_status()\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])\n",
        "\n",
        "        time.sleep(1) #to avoid error 429 : too many requests in short time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n1qGVyRVCJN2",
      "metadata": {
        "id": "n1qGVyRVCJN2"
      },
      "outputs": [],
      "source": [
        "category = \"withdrawn\" #[\"accepted(oral)\", \"accepted(poster)\", \"accepted(spotlight)\", \"desk_rejected\", \"rejected\", \"withdrawn\"]\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICLR/ICLR 2025/{category}\")\n",
        "json_count = len(list(directory.glob(\"*.json\")))\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/ICLR/ICLR 2025/{category}/{category}_arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "    for i in range(1, json_count+1):\n",
        "      #change json name with every category update\n",
        "      with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICLR/ICLR 2025/{category}/ICLR2025_{category}_page{i}.json\", 'r') as file:\n",
        "        all_content = json.load(file)\n",
        "        for note in all_content['notes']:\n",
        "          paper_title = note['content']['title']['value']\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          #error 400\n",
        "          if response.status_code == 400:\n",
        "            continue\n",
        "\n",
        "          #to avoid error 429 : too many requests in short time\n",
        "          if response.status_code == 429:\n",
        "              print(\"Rate limit hit. Waiting...\")\n",
        "              time.sleep(10)  # Wait 10 seconds before retry\n",
        "              response = requests.get(search_query)\n",
        "          response.raise_for_status()\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])\n",
        "\n",
        "        time.sleep(1) #to avoid error 429 : too many requests in short time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sRRYGjVSuThb",
      "metadata": {
        "id": "sRRYGjVSuThb"
      },
      "source": [
        "## ICML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "enm5HMx9ukEj",
      "metadata": {
        "id": "enm5HMx9ukEj"
      },
      "outputs": [],
      "source": [
        "category = \"accept(oral)\" #[\"accept(poster)\", \"accept(oral), \"accept(spotlight)\", \"rejected\", \"retracted acceptance\"]\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICML/ICML 2025/{category}\")\n",
        "json_count = len(list(directory.glob(\"*.json\")))\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/ICML/ICML 2025/{category}/{category}_arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "    for i in range(1, json_count+1):\n",
        "      #change json name with every category update\n",
        "      with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICML/ICML 2025/{category}/ICML2025_oral_page{i}.json\", 'r') as file:\n",
        "        all_content = json.load(file)\n",
        "        for note in all_content['notes']:\n",
        "          paper_title = note['content']['title']['value']\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          #error 400\n",
        "          if response.status_code == 400:\n",
        "            continue\n",
        "\n",
        "          #to avoid error 429 : too many requests in short time\n",
        "          if response.status_code == 429:\n",
        "              print(\"Rate limit hit. Waiting...\")\n",
        "              time.sleep(10)  # Wait 10 seconds before retry\n",
        "              response = requests.get(search_query)\n",
        "          response.raise_for_status()\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])\n",
        "\n",
        "        time.sleep(1) #to avoid error 429 : too many requests in short time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UAcCqf0s5AgU",
      "metadata": {
        "id": "UAcCqf0s5AgU"
      },
      "outputs": [],
      "source": [
        "category = \"accept(spotlight)\" #[\"accept(poster)\", \"accept(oral), \"accept(spotlight)\", \"rejected\", \"retracted acceptance\"]\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICML/ICML 2025/{category}\")\n",
        "json_count = len(list(directory.glob(\"*.json\")))\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/ICML/ICML 2025/{category}/{category}_arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "    for i in range(1, json_count+1):\n",
        "      #change json name with every category update\n",
        "      with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICML/ICML 2025/{category}/ICML2025_spotlight_page{i}.json\", 'r') as file:\n",
        "        all_content = json.load(file)\n",
        "        for note in all_content['notes']:\n",
        "          paper_title = note['content']['title']['value']\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          #error 400\n",
        "          if response.status_code == 400:\n",
        "            continue\n",
        "\n",
        "          #to avoid error 429 : too many requests in short time\n",
        "          if response.status_code == 429:\n",
        "              print(\"Rate limit hit. Waiting...\")\n",
        "              time.sleep(10)  # Wait 10 seconds before retry\n",
        "              response = requests.get(search_query)\n",
        "          response.raise_for_status()\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])\n",
        "\n",
        "        time.sleep(1) #to avoid error 429 : too many requests in short time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4LcdO8o05DYO",
      "metadata": {
        "id": "4LcdO8o05DYO"
      },
      "outputs": [],
      "source": [
        "category = \"accept(poster)\" #[\"accept(poster)\", \"accept(oral), \"accept(spotlight)\", \"rejected\", \"retracted acceptance\"]\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICML/ICML 2025/{category}\")\n",
        "json_count = len(list(directory.glob(\"*.json\")))\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/ICML/ICML 2025/{category}/{category}_arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "    for i in range(1, json_count+1):\n",
        "      #change json name with every category update\n",
        "      with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICML/ICML 2025/{category}/ICML2025_poster_page{i}.json\", 'r') as file:\n",
        "        all_content = json.load(file)\n",
        "        for note in all_content['notes']:\n",
        "          paper_title = note['content']['title']['value']\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          #error 400\n",
        "          if response.status_code == 400:\n",
        "            continue\n",
        "\n",
        "          #to avoid error 429 : too many requests in short time\n",
        "          if response.status_code == 429:\n",
        "              print(\"Rate limit hit. Waiting...\")\n",
        "              time.sleep(10)  # Wait 10 seconds before retry\n",
        "              response = requests.get(search_query)\n",
        "          response.raise_for_status()\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])\n",
        "\n",
        "        time.sleep(1) #to avoid error 429 : too many requests in short time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gBC9MCuW7Q9P",
      "metadata": {
        "id": "gBC9MCuW7Q9P"
      },
      "outputs": [],
      "source": [
        "category = \"rejected\" #[\"accept(poster)\", \"accept(oral), \"accept(spotlight)\", \"rejected\", \"retracted acceptance\"]\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICML/ICML 2025/{category}\")\n",
        "json_count = len(list(directory.glob(\"*.json\")))\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/ICML/ICML 2025/{category}/{category}_arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "    for i in range(1, json_count+1):\n",
        "      #change json name with every category update\n",
        "      with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICML/ICML 2025/{category}/ICML2025_{category}_page{i}.json\", 'r') as file:\n",
        "        all_content = json.load(file)\n",
        "        for note in all_content['notes']:\n",
        "          paper_title = note['content']['title']['value']\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          #error 400\n",
        "          if response.status_code == 400:\n",
        "            continue\n",
        "\n",
        "          #to avoid error 429 : too many requests in short time\n",
        "          if response.status_code == 429:\n",
        "              print(\"Rate limit hit. Waiting...\")\n",
        "              time.sleep(10)  # Wait 10 seconds before retry\n",
        "              response = requests.get(search_query)\n",
        "          response.raise_for_status()\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])\n",
        "\n",
        "        time.sleep(1) #to avoid error 429 : too many requests in short time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sWkSthzU7RHZ",
      "metadata": {
        "id": "sWkSthzU7RHZ"
      },
      "outputs": [],
      "source": [
        "category = \"retracted acceptance\" #[\"accept(poster)\", \"accept(oral), \"accept(spotlight)\", \"rejected\", \"retracted acceptance\"]\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICML/ICML 2025/{category}\")\n",
        "json_count = len(list(directory.glob(\"*.json\")))\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/ICML/ICML 2025/{category}/{category}_arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "    for i in range(1, json_count+1):\n",
        "      #change json name with every category update\n",
        "      with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/ICML/ICML 2025/{category}/ICML2025_retracted_page{i}.json\", 'r') as file:\n",
        "        all_content = json.load(file)\n",
        "        for note in all_content['notes']:\n",
        "          paper_title = note['content']['title']['value']\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          #error 400\n",
        "          if response.status_code == 400:\n",
        "            continue\n",
        "\n",
        "          #to avoid error 429 : too many requests in short time\n",
        "          if response.status_code == 429:\n",
        "              print(\"Rate limit hit. Waiting...\")\n",
        "              time.sleep(10)  # Wait 10 seconds before retry\n",
        "              response = requests.get(search_query)\n",
        "          response.raise_for_status()\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])\n",
        "\n",
        "        time.sleep(1) #to avoid error 429 : too many requests in short time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0PLBinHUukrF",
      "metadata": {
        "id": "0PLBinHUukrF"
      },
      "source": [
        "## NeurIPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UBvJbtOX-UCc",
      "metadata": {
        "id": "UBvJbtOX-UCc"
      },
      "outputs": [],
      "source": [
        "category = \"accept(poster)\" #[\"accept(poster)\", \"accept(oral), \"accept(spotlight)\", \"reject\"]\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/NeurIPS/NeurIPS 2023/{category}\")\n",
        "json_count = len(list(directory.glob(\"*.json\")))\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/NeurIPS/NeurIPS 2023/{category}/{category}_arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "    for i in range(1, json_count+1):\n",
        "      #change json name with every category update\n",
        "      with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/NeurIPS/NeurIPS 2023/{category}/NeurIPS2023_poster_page{i}.json\", 'r') as file:\n",
        "        all_content = json.load(file)\n",
        "        for note in all_content['notes']:\n",
        "          paper_title = note['content']['title']['value']\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          #error 400\n",
        "          if response.status_code == 400:\n",
        "            continue\n",
        "\n",
        "          #to avoid error 429 : too many requests in short time\n",
        "          if response.status_code == 429:\n",
        "              print(\"Rate limit hit. Waiting...\")\n",
        "              time.sleep(10)  # Wait 10 seconds before retry\n",
        "              response = requests.get(search_query)\n",
        "          response.raise_for_status()\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])\n",
        "\n",
        "        time.sleep(1) #to avoid error 429 : too many requests in short time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2kv-IRgoDEfS",
      "metadata": {
        "id": "2kv-IRgoDEfS"
      },
      "outputs": [],
      "source": [
        "category = \"accept(poster)\" #[\"accept(poster)\", \"accept(oral), \"accept(spotlight)\", \"reject\"]\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/NeurIPS/NeurIPS 2024/{category}\")\n",
        "json_count = len(list(directory.glob(\"*.json\")))\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/NeurIPS/NeurIPS 2024/{category}/{category}_arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "    for i in range(1, json_count+1):\n",
        "      #change json name with every category update\n",
        "      with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/NeurIPS/NeurIPS 2024/{category}/NeurIPS2024_poster_page{i}.json\", 'r') as file:\n",
        "        all_content = json.load(file)\n",
        "        for note in all_content['notes']:\n",
        "          paper_title = note['content']['title']['value']\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          #error 400\n",
        "          if response.status_code == 400:\n",
        "            continue\n",
        "\n",
        "          #to avoid error 429 : too many requests in short time\n",
        "          if response.status_code == 429:\n",
        "              print(\"Rate limit hit. Waiting...\")\n",
        "              time.sleep(10)  # Wait 10 seconds before retry\n",
        "              response = requests.get(search_query)\n",
        "          response.raise_for_status()\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])\n",
        "\n",
        "        time.sleep(1) #to avoid error 429 : too many requests in short time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3vceg9vvBu4L",
      "metadata": {
        "id": "3vceg9vvBu4L"
      },
      "outputs": [],
      "source": [
        "category = \"accept(oral)\" #[\"accept(poster)\", \"accept(oral), \"accept(spotlight)\", \"reject\"]\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/NeurIPS/NeurIPS 2023/{category}\")\n",
        "json_count = len(list(directory.glob(\"*.json\")))\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/NeurIPS/NeurIPS 2023/{category}/{category}_arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "    for i in range(1, json_count+1):\n",
        "      #change json name with every category update\n",
        "      with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/NeurIPS/NeurIPS 2023/{category}/NeurIPS2023_oral_page{i}.json\", 'r') as file:\n",
        "        all_content = json.load(file)\n",
        "        for note in all_content['notes']:\n",
        "          paper_title = note['content']['title']['value']\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          #error 400\n",
        "          if response.status_code == 400:\n",
        "            continue\n",
        "\n",
        "          #to avoid error 429 : too many requests in short time\n",
        "          if response.status_code == 429:\n",
        "              print(\"Rate limit hit. Waiting...\")\n",
        "              time.sleep(10)  # Wait 10 seconds before retry\n",
        "              response = requests.get(search_query)\n",
        "          response.raise_for_status()\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])\n",
        "\n",
        "        time.sleep(1) #to avoid error 429 : too many requests in short time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ALcVlJNbDFY7",
      "metadata": {
        "id": "ALcVlJNbDFY7"
      },
      "outputs": [],
      "source": [
        "category = \"accept(oral)\" #[\"accept(poster)\", \"accept(oral), \"accept(spotlight)\", \"reject\"]\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/NeurIPS/NeurIPS 2024/{category}\")\n",
        "json_count = len(list(directory.glob(\"*.json\")))\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/NeurIPS/NeurIPS 2024/{category}/{category}_arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "    for i in range(1, json_count+1):\n",
        "      #change json name with every category update\n",
        "      with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/NeurIPS/NeurIPS 2024/{category}/NeurIPS2024_oral_page{i}.json\", 'r') as file:\n",
        "        all_content = json.load(file)\n",
        "        for note in all_content['notes']:\n",
        "          paper_title = note['content']['title']['value']\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          #error 400\n",
        "          if response.status_code == 400:\n",
        "            continue\n",
        "\n",
        "          #to avoid error 429 : too many requests in short time\n",
        "          if response.status_code == 429:\n",
        "              print(\"Rate limit hit. Waiting...\")\n",
        "              time.sleep(10)  # Wait 10 seconds before retry\n",
        "              response = requests.get(search_query)\n",
        "          response.raise_for_status()\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])\n",
        "\n",
        "        time.sleep(1) #to avoid error 429 : too many requests in short time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ReFGdeRBvDl",
      "metadata": {
        "id": "3ReFGdeRBvDl"
      },
      "outputs": [],
      "source": [
        "category = \"accept(spotlight)\" #[\"accept(poster)\", \"accept(oral), \"accept(spotlight)\", \"reject\"]\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/NeurIPS/NeurIPS 2023/{category}\")\n",
        "json_count = len(list(directory.glob(\"*.json\")))\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/NeurIPS/NeurIPS 2023/{category}/{category}_arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "    for i in range(1, json_count+1):\n",
        "      #change json name with every category update\n",
        "      with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/NeurIPS/NeurIPS 2023/{category}/NeurIPS2023_spotlight_page{i}.json\", 'r') as file:\n",
        "        all_content = json.load(file)\n",
        "        for note in all_content['notes']:\n",
        "          paper_title = note['content']['title']['value']\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          #error 400\n",
        "          if response.status_code == 400:\n",
        "            continue\n",
        "\n",
        "          #to avoid error 429 : too many requests in short time\n",
        "          if response.status_code == 429:\n",
        "              print(\"Rate limit hit. Waiting...\")\n",
        "              time.sleep(10)  # Wait 10 seconds before retry\n",
        "              response = requests.get(search_query)\n",
        "          response.raise_for_status()\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])\n",
        "\n",
        "        time.sleep(1) #to avoid error 429 : too many requests in short time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mULrvz_wDGPP",
      "metadata": {
        "id": "mULrvz_wDGPP"
      },
      "outputs": [],
      "source": [
        "category = \"accept(spotlight)\" #[\"accept(poster)\", \"accept(oral), \"accept(spotlight)\", \"reject\"]\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/NeurIPS/NeurIPS 2024/{category}\")\n",
        "json_count = len(list(directory.glob(\"*.json\")))\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/NeurIPS/NeurIPS 2024/{category}/{category}_arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "    for i in range(1, json_count+1):\n",
        "      #change json name with every category update\n",
        "      with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/NeurIPS/NeurIPS 2024/{category}/NeurIPS2024_spotlight_page{i}.json\", 'r') as file:\n",
        "        all_content = json.load(file)\n",
        "        for note in all_content['notes']:\n",
        "          paper_title = note['content']['title']['value']\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          #error 400\n",
        "          if response.status_code == 400:\n",
        "            continue\n",
        "\n",
        "          #to avoid error 429 : too many requests in short time\n",
        "          if response.status_code == 429:\n",
        "              print(\"Rate limit hit. Waiting...\")\n",
        "              time.sleep(10)  # Wait 10 seconds before retry\n",
        "              response = requests.get(search_query)\n",
        "          response.raise_for_status()\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])\n",
        "\n",
        "        time.sleep(1) #to avoid error 429 : too many requests in short time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ANxvHLqPBvLL",
      "metadata": {
        "id": "ANxvHLqPBvLL"
      },
      "outputs": [],
      "source": [
        "category = \"reject\" #[\"accept(poster)\", \"accept(oral), \"accept(spotlight)\", \"reject\"]\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/NeurIPS/NeurIPS 2023/{category}\")\n",
        "json_count = len(list(directory.glob(\"*.json\")))\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/NeurIPS/NeurIPS 2023/{category}/{category}_arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "    for i in range(1, json_count+1):\n",
        "      #change json name with every category update\n",
        "      with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/NeurIPS/NeurIPS 2023/{category}/NeurIPS2023_{category}_page{i}.json\", 'r') as file:\n",
        "        all_content = json.load(file)\n",
        "        for note in all_content['notes']:\n",
        "          paper_title = note['content']['title']['value']\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          #error 400\n",
        "          if response.status_code == 400:\n",
        "            continue\n",
        "\n",
        "          #to avoid error 429 : too many requests in short time\n",
        "          if response.status_code == 429:\n",
        "              print(\"Rate limit hit. Waiting...\")\n",
        "              time.sleep(10)  # Wait 10 seconds before retry\n",
        "              response = requests.get(search_query)\n",
        "          response.raise_for_status()\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])\n",
        "\n",
        "        time.sleep(1) #to avoid error 429 : too many requests in short time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i8qD3yGjC-k4",
      "metadata": {
        "id": "i8qD3yGjC-k4"
      },
      "outputs": [],
      "source": [
        "category = \"reject\" #[\"accept(poster)\", \"accept(oral), \"accept(spotlight)\", \"reject\"]\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/NeurIPS/NeurIPS 2024/{category}\")\n",
        "json_count = len(list(directory.glob(\"*.json\")))\n",
        "with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/NeurIPS/NeurIPS 2024/{category}/{category}_arxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "    for i in range(1, json_count+1):\n",
        "      #change json name with every category update\n",
        "      with open(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/NeurIPS/NeurIPS 2024/{category}/NeurIPS2024_{category}_page{i}.json\", 'r') as file:\n",
        "        all_content = json.load(file)\n",
        "        for note in all_content['notes']:\n",
        "          paper_title = note['content']['title']['value']\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          #error 400\n",
        "          if response.status_code == 400:\n",
        "            continue\n",
        "\n",
        "          #to avoid error 429 : too many requests in short time\n",
        "          if response.status_code == 429:\n",
        "              print(\"Rate limit hit. Waiting...\")\n",
        "              time.sleep(10)  # Wait 10 seconds before retry\n",
        "              response = requests.get(search_query)\n",
        "          response.raise_for_status()\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])\n",
        "\n",
        "        time.sleep(1) #to avoid error 429 : too many requests in short time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RKNnDUEIA2H5",
      "metadata": {
        "id": "RKNnDUEIA2H5"
      },
      "source": [
        "## EMNLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba8yoKvMRbF2",
      "metadata": {
        "id": "ba8yoKvMRbF2"
      },
      "outputs": [],
      "source": [
        "year = 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m8UMeBFtRbel",
      "metadata": {
        "id": "m8UMeBFtRbel"
      },
      "outputs": [],
      "source": [
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/EMNLP/{year}\")\n",
        "output_dir = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/EMNLP/{year}\")\n",
        "csv_files = list(directory.glob(\"*.csv\"))\n",
        "for file in csv_files:\n",
        "  df = pd.read_csv(file)\n",
        "  titles = df[\"title\"]\n",
        "\n",
        "  output_csv = output_dir / f\"{file.stem}_arxiv.csv\"\n",
        "  with open (output_csv, \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "\n",
        "    for i in range (len(titles)):\n",
        "        paper_title = titles[i]\n",
        "        paper = paper_title.replace(\" \",\"+\")\n",
        "        search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "        response = requests.get(search_query)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # <p> tag that contains the submission info\n",
        "        submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "        date = \"Not Found\"\n",
        "\n",
        "        if submission_info:\n",
        "            text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "            # find v1 submitted date first\n",
        "            v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "            if v1_match:\n",
        "                date = v1_match.group(1).strip()\n",
        "            else:\n",
        "                # If not found, originally announced date\n",
        "                announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                if announced_match:\n",
        "                    date = announced_match.group(1).strip()\n",
        "\n",
        "        writer.writerow([paper_title, date])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CQ5IGs0vX9Zr",
      "metadata": {
        "id": "CQ5IGs0vX9Zr"
      },
      "outputs": [],
      "source": [
        "year = 2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7YrZ8Jw9X9qx",
      "metadata": {
        "id": "7YrZ8Jw9X9qx"
      },
      "outputs": [],
      "source": [
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/EMNLP/{year}\")\n",
        "csv_files = list(directory.glob(\"*.csv\"))\n",
        "for file in csv_files:\n",
        "  df = pd.read_csv(file)\n",
        "  titles = df[\"title\"]\n",
        "  output_dir = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/EMNLP/{year}\")\n",
        "  output_csv = output_dir / f\"{file.stem}_arxiv.csv\"\n",
        "  with open (output_csv, \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "\n",
        "    for i in range (len(titles)):\n",
        "        paper_title = titles[i]\n",
        "        paper = paper_title.replace(\" \",\"+\")\n",
        "        search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "        response = requests.get(search_query)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # <p> tag that contains the submission info\n",
        "        submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "        date = \"Not Found\"\n",
        "\n",
        "        if submission_info:\n",
        "            text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "            # find v1 submitted date first\n",
        "            v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "            if v1_match:\n",
        "                date = v1_match.group(1).strip()\n",
        "            else:\n",
        "                # If not found, originally announced date\n",
        "                announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                if announced_match:\n",
        "                    date = announced_match.group(1).strip()\n",
        "\n",
        "        writer.writerow([paper_title, date])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  df = pd.read_csv(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/EMNLP/2025/EMNLP2025.csv\")\n",
        "  titles = df[\"title\"].to_list()\n",
        "  with open (f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/EMNLP/2025/EMNLP2025arxivarxiv.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "      writer=csv.writer(f)\n",
        "      writer.writerow([\"Title\",\"Submission Date\"])\n",
        "\n",
        "      for i in range (len(titles)):\n",
        "          paper_title = titles[i]\n",
        "          paper = paper_title.replace(\" \",\"+\")\n",
        "          search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "          response = requests.get(search_query)\n",
        "          soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "          # <p> tag that contains the submission info\n",
        "          submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "          date = \"Not Found\"\n",
        "\n",
        "          if submission_info:\n",
        "              text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "              # find v1 submitted date first\n",
        "              v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "              if v1_match:\n",
        "                  date = v1_match.group(1).strip()\n",
        "              else:\n",
        "                  # If not found, originally announced date\n",
        "                  announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                  if announced_match:\n",
        "                      date = announced_match.group(1).strip()\n",
        "\n",
        "          writer.writerow([paper_title, date])"
      ],
      "metadata": {
        "id": "FmlxxHm96cy0"
      },
      "id": "FmlxxHm96cy0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "26TOqvC1QNmn",
      "metadata": {
        "id": "26TOqvC1QNmn"
      },
      "source": [
        "## COLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LKH68d5RQMio",
      "metadata": {
        "id": "LKH68d5RQMio"
      },
      "outputs": [],
      "source": [
        "year = 2025\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/COLING/{year}\")\n",
        "csv_files = list(directory.glob(\"*.csv\"))\n",
        "for file in csv_files:\n",
        "  df = pd.read_csv(file)\n",
        "  titles = df[\"title\"]\n",
        "  output_dir = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/COLING/{year}\")\n",
        "  output_csv = output_dir / f\"{file.stem}_arxiv.csv\"\n",
        "  with open (output_csv, \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "\n",
        "    for i in range (len(titles)):\n",
        "        paper_title = titles[i]\n",
        "        paper = paper_title.replace(\" \",\"+\")\n",
        "        search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "        response = requests.get(search_query)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # <p> tag that contains the submission info\n",
        "        submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "        date = \"Not Found\"\n",
        "\n",
        "        if submission_info:\n",
        "            text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "            # find v1 submitted date first\n",
        "            v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "            if v1_match:\n",
        "                date = v1_match.group(1).strip()\n",
        "            else:\n",
        "                # If not found, originally announced date\n",
        "                announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                if announced_match:\n",
        "                    date = announced_match.group(1).strip()\n",
        "\n",
        "        writer.writerow([paper_title, date])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H1p1fVu8pZdV",
      "metadata": {
        "id": "H1p1fVu8pZdV"
      },
      "outputs": [],
      "source": [
        "year = 2024\n",
        "directory = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Conferences/COLING/{year}\")\n",
        "csv_files = list(directory.glob(\"*.csv\"))\n",
        "for file in csv_files:\n",
        "  df = pd.read_csv(file)\n",
        "  titles = df[\"Title\"]\n",
        "  output_dir = Path(f\"/content/drive/MyDrive/Project Progress/Generated Dataset/Arxiv/COLING/{year}\")\n",
        "  output_csv = output_dir / f\"{file.stem}_arxiv.csv\"\n",
        "  with open (output_csv, \"w\", newline = \"\", encoding = \"utf-8\") as f:\n",
        "    writer=csv.writer(f)\n",
        "    writer.writerow([\"Title\",\"Submission Date\"])\n",
        "\n",
        "    for i in range (len(titles)):\n",
        "        paper_title = titles[i]\n",
        "        paper = paper_title.replace(\" \",\"+\")\n",
        "        search_query = f\"https://arxiv.org/search/?query={paper}&searchtype=title&order=-announced_date_first\"\n",
        "        response = requests.get(search_query)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # <p> tag that contains the submission info\n",
        "        submission_info = soup.find(\"p\", class_=\"is-size-7\")\n",
        "        date = \"Not Found\"\n",
        "\n",
        "        if submission_info:\n",
        "            text = submission_info.get_text(\" \", strip=True)\n",
        "\n",
        "            # find v1 submitted date first\n",
        "            v1_match = re.search(r\"v1 submitted ([^;]+);\", text)\n",
        "            if v1_match:\n",
        "                date = v1_match.group(1).strip()\n",
        "            else:\n",
        "                # If not found, originally announced date\n",
        "                announced_match = re.search(r\"originally announced ([^.]+)\\.\", text)\n",
        "                if announced_match:\n",
        "                    date = announced_match.group(1).strip()\n",
        "\n",
        "        writer.writerow([paper_title, date])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "e04dabe4-61e0-4c68-9b4f-00cac8c73192",
        "INxgSLLiFg0G",
        "h4b7Rv8Xxh9y",
        "YfFK7Y3L8nCJ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}